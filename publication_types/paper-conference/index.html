<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: July 6, 2024 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.2.0" />

  
  












  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Xia Li" />

  
  
  
    
  
  <meta name="description" content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website." />

  
  <link rel="alternate" hreflang="en-us" href="https://xialipku.github.io/publication_types/paper-conference/" />

  
  
  
  
    
    <link rel="stylesheet" href="/css/themes/blue.min.css" />
  

  
  
    
    <link href="/dist/wc.min.css" rel="stylesheet" />
  

  
  
  

  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  












<script async src="https://www.googletagmanager.com/gtag/js?id=LF6EHN32R3"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'LF6EHN32R3', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>



























  
  
    <link rel="alternate" href="/publication_types/paper-conference/index.xml" type="application/rss+xml" title="Xia Li" />
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://xialipku.github.io/publication_types/paper-conference/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@GetResearchDev" />
    <meta property="twitter:creator" content="@GetResearchDev" />
  
  <meta property="og:site_name" content="Xia Li" />
  <meta property="og:url" content="https://xialipku.github.io/publication_types/paper-conference/" />
  <meta property="og:title" content="Paper-Conference | Xia Li" />
  <meta property="og:description" content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website." /><meta property="og:image" content="https://xialipku.github.io/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://xialipku.github.io/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2024-07-08T00:00:00&#43;00:00" />
    
  

  




  <title>Paper-Conference | Xia Li</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  







<link type="text/css" rel="stylesheet" href="/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css" integrity="sha256-vnZutBkxehTsdp0hbpd5v&#43;jzc3yA54D0ug2vtXpBpII=" />


<script src="/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js" integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script>


<script>window.hbb.pagefind = {"baseUrl":"/"};</script>

<style>
  html.dark {
    --pagefind-ui-primary: #eeeeee;
    --pagefind-ui-text: #eeeeee;
    --pagefind-ui-background: #152028;
    --pagefind-ui-border: #152028;
    --pagefind-ui-tag: #152028;
  }
</style>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    new PagefindUI({
      element: "#search",
      showSubResults: true,
      baseUrl: window.hbb.pagefind.baseUrl,
      bundlePath: window.hbb.pagefind.baseUrl + "pagefind/",
    });
  });
  document.addEventListener('DOMContentLoaded', () => {
    let element = document.getElementById('search');
    let trigger = document.getElementById('search_toggle');

    if (trigger) {
      trigger.addEventListener('click', () => {
        element.classList.toggle('hidden');
        element.querySelector("input").value = ""
        element.querySelector("input").focus()

        if (!element.classList.contains('hidden')) {
          let clear_trigger = document.querySelector('.pagefind-ui__search-clear');

          if (clear_trigger && !clear_trigger.hasAttribute('listenerOnClick')) {
            clear_trigger.setAttribute('listenerOnClick', 'true');

            clear_trigger.addEventListener('click', () => {
              element.classList.toggle('hidden');
            });
          }
        }

      });
    }
  });
</script>















  
  
  
  
  
  
  
  <script
    defer
    src="/js/hugo-blox-en.min.e5fa931947cac2d947732ea37a770aae2b5bd4a50b6048060cd129b46159a06d.js"
    integrity="sha256-5fqTGUfKwtlHcy6jencKritb1KULYEgGDNEptGFZoG0="
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>

  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/" title="Xia Li">
        Xia Li
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/"
        >Bio</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#news"
        >News</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/publication"
        >Publications</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/event"
        >Talks</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/experience/"
        >Experience</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/awards/"
        >Awards</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      
      <button
        aria-label="search"
        class="text-black hover:text-primary  inline-block px-3 text-xl dark:text-white"
        id="search_toggle">
        <svg xmlns="http://www.w3.org/2000/svg" height="16" width="16" viewBox="0 0 512 512" fill="currentColor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352a144 144 0 1 0 0-288 144 144 0 1 0 0 288z"/></svg>
      </button>
      

      
      
      <div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
            [&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white">
        <button class="theme-toggle mt-1" accesskey="t" title="appearance">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class="dark:hidden">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class=" dark:block [&:not(dark)]:hidden">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
      

      
      

      
      
    </div>
  </nav>
</header>


<div id="search" class="hidden p-3"></div>


        
      
    </div>
    <div class="page-body  my-10">
      

<div class="max-w-prose mx-auto flex justify-center">
  <article class="prose prose-slate lg:prose-xl dark:prose-invert">
    <h1 class="lg:text-6xl">Paper-Conference</h1>
    
  </article>
</div>




<div class="flex flex-col items-center">

  <div class="container max-w-[65ch] mx-auto bg-white dark:bg-zinc-900 rounded-xl border-gray-100 dark:border-gray-700 border shadow-md overflow-hidden my-5">


  
  
  








<a href="/publication/iccr24-ngp-dir/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/iccr24-ngp-dir/featured_hu96fe106805ab412538f27a974e3cf4c4_377606_2a234543f044e8f08f88faac538f473d.webp" height="655" width="655" alt="Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this study, we have successfully integrated NGP into DIR, a novel contribution that significantly enhances the accuracy and efficiency of medical image alignment as demonstrated on the DIR-lab dataset. The NGPDIR framework exhibits robust performance across various metrics, particularly in landmark alignment precision and the accommodation of anatomical sliding boundaries. This advancement not only propels the DIR field forward but also opens new avenues for real-time clinical applications, potentially transforming patient care with its rapid, reliable imaging capabilities.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 8, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/cvpr24-icl-pose/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/cvpr24-icl-pose/featured_hu4d9ec96548c27c65612f16aed24d56cd_238333_d232fe1d3bb9f617c13e9a4b9faed1db.webp" height="655" width="655" alt="Skeleton-in-context: Unified skeleton sequence modeling with in-context learning">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Skeleton-in-context: Unified skeleton sequence modeling with in-context learning</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this work, we propose the Skeleton-in-Context, designed to process multiple skeleton-base tasks simultaneously after just one training time. Specifically, we build a novel skeleton-based in-context benchmark covering various tasks. In particular, we propose skeleton prompts composed of TGP and TUP, which solve the overfitting problem of skeleton sequence data trained under the training framework commonly applied in previous 2D and 3D in-context models. Besides, we demonstrate that our model can generalize to different datasets and new tasks, such as motion completion. We hope our research builds the first step in the exploration of in-context learning for skeleton-based sequences, which paves the way for further research in this area.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Dec 15, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/neurips23-icl-point/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/neurips23-icl-point/featured_huc09ae552efe385e96289ecf777809b96_472423_0c92884518cfab477d5947240d03462f.webp" height="655" width="655" alt="Explore In-Context Learning for 3D Point Cloud Understanding">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Explore In-Context Learning for 3D Point Cloud Understanding</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose Point-In-Context (PIC), the first framework adopting the in-context learning paradigm for 3D point cloud understanding. Specifically, we set up an extensive dataset of point cloud pairs with four fundamental tasks to achieve in-context ability. We propose effective designs that facilitate the training and solve the inherited information leakage problem. PIC shows its excellent learning capacity, achieves comparable results with single-task models, and outperforms multitask models on all four tasks. Besides, it shows good generalization ability to out-of-distribution samples and unseen tasks and has great potential via selecting higher-quality prompts. We hope it paves the way for further exploration of in-context learning in the 3D modalities.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Dec 10, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/iccv23-coevo-mesh/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/iccv23-coevo-mesh/featured_hu6360c533b6c06cede09efa83e846afd5_447307_1b13ee32298dc18d92fd51212346c948.webp" height="655" width="655" alt="Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        This paper proposes the Pose and Mesh Co-Evolution network (PMCE), a new two-stage pose-to-mesh framework for recovering 3D human mesh from a monocular video. PMCE frst estimates 3D human pose motion in terms of spatial and temporal domains, then performs image-guided pose and mesh interactions by our proposed AdaLN that injects body shape information while preserving their spatial structure. Extensive experiments on popular datasets show that PMCE outperforms state-of-the-art methods in both perframe accuracy and temporal consistency. We hope that our approach will spark further research in 3D human motion estimation considering both pose and shape consistency.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Oct 2, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/iccv23-caption-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/iccv23-caption-seg/featured_hu011ea7685f847792f4a9d7cc2f95615e_389155_22c77cfa92b09631b702288094ee7bc9.webp" height="655" width="655" alt="Betrayed by captions: Joint caption grounding and generation for open vocabulary instance segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Betrayed by captions: Joint caption grounding and generation for open vocabulary instance segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        This paper presents a joint Caption Grounding and Generation (CGG) framework for instance-level open vocabulary segmentation. The main contributions are: (1) using fine-grained object nouns in captions to improve grounding with object queries. (2) using captions as supervision signals to extract rich information from other words helps identify novel categories. To our knowledge, this paper is the first to unify segmentation and caption generation for open vocabulary learning. The proposed framework significantly improves OVIS and OSPS and comparable results on OVOD without pre-training on large-scale datasets.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Oct 2, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/neurips21-proto-track-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/neurips21-proto-track-seg/featured_hucb35216e12caf65b285e8177fe5d0540_381581_3b16783e3fb1a58fcc4311f346ab2bd8.webp" height="655" width="655" alt="Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich spatio-temporal information for online multiple object tracking and segmentation. </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Sep 12, 2021</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/cvpr21-quasi-track/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/cvpr21-quasi-track/featured_huf1361c5b795c8badd6f2979bf92e6920_1106383_8f05b824185fefba4ef9c099edf5ead6.webp" height="655" width="655" alt="Quasi-dense similarity learning for multiple object tracking">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Quasi-dense similarity learning for multiple object tracking</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We present Quasi-Dense Similarity Learning, which densely samples hundreds of region proposals on a pair of images for contrastive learning.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 20, 2021</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/cvpr21-point-flow-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/cvpr21-point-flow-seg/featured_hu309c9fb14b8abc1ebff95577a488bf34_593424_06f8df259df6fba4b00a52b4acb9163a.webp" height="655" width="655" alt="PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 13, 2021</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/iclr20-md-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/iclr20-md-seg/featured_hue037f6df33459d6147d3b335be2799d9_74977_fc088931a148c4104916f66071749b6b.webp" height="655" width="655" alt="Is Attention Better Than Matrix Decomposition?">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Is Attention Better Than Matrix Decomposition?</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Self-attention is not better than the matrix decomposition~(MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Sep 28, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/eccv20-decouple-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/eccv20-decouple-seg/featured_huc6030c583a6246ba24d8ca663433c0d8_604507_7cff5f71aed8efed1785aced0f7540e4.webp" height="655" width="655" alt="Improving Semantic Segmentation via Decoupled Body and Edge Supervision">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Improving Semantic Segmentation via Decoupled Body and Edge Supervision</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 3, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/cvpr20-spygr-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/cvpr20-spygr-seg/featured_hu43d2a3705a01344ae35e6916ebaa3685_689664_581ba5760dba1162fb3718d933d4687d.webp" height="655" width="655" alt="Spatial Pyramid Based Graph Reasoning for Semantic Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Spatial Pyramid Based Graph Reasoning for Semantic Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 24, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/aaai20-sognet-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/aaai20-sognet-seg/featured_hu559f28cda505009a79e68e1e21f4a288_271305_fec9a3b8d7933a278d52e017fb506aeb.webp" height="655" width="655" alt="SOGNet: Scene Overlap Graph Network for Panoptic Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">SOGNet: Scene Overlap Graph Network for Panoptic Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We leverage each object’s category, geometry and appearance features to perform relational embedding, and output a relation matrix that encodes overlap relations. In order to overcome the lack of supervision, we introduce a differentiable module to resolve the overlap between any pair of instances.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 7, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/aaai20-tcs-cls/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/aaai20-tcs-cls/featured_huaf3a0810c4a3c1ca5a417386bed61ce5_46007_0621976457504149618aaa1bdb321f6b.webp" height="655" width="655" alt="Dynamic System Inspired Adaptive Time Stepping Controller for Residual Networks Families">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Dynamic System Inspired Adaptive Time Stepping Controller for Residual Networks Families</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We analyze the effects of time stepping on the Euler method and ResNets. We establish a stability condition for ResNets with step sizes and weight parameters, and point out the effects of step sizes on the stability and performance. Inspired by our analyses, we develop an adaptive time stepping controller that is dependent on the parameters of the current step, and aware of previous steps.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 7, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/iccv19-emanet-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/iccv19-emanet-seg/featured_hu9697392da5f15752eba3cc9c3d5fcfba_326134_a8ee754e23f7d202928bfee741e7e101.webp" height="655" width="655" alt="Expectation Maximization Attention Networks for Semantic Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Expectation Maximization Attention Networks for Semantic Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 22, 2019</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/miccai19-r2net-destreak/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/miccai19-r2net-destreak/featured_hudf37fe5dd93a0851274cb8b6242e8abb_83498_bb0ba89bd3468194d54d149468d2662f.webp" height="655" width="655" alt="R^2 Net Recurrent and Recursive Network for Sparse View CT Artifacts Removal">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">R^2 Net Recurrent and Recursive Network for Sparse View CT Artifacts Removal</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose a novel neural network architecture to reduce streak artifacts generated in sparse-view 2D Cone Beam Computed To-mography (CBCT) image reconstruction.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jun 19, 2019</p>
    </div>
  </div>
</a>

  
  








<a href="/publication/eccv18-rescan-derain/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/publication/eccv18-rescan-derain/featured_hu50977331c69894a416aa004775fc30b5_546954_6377088a99e07ccadaac1471ab3f4135.webp" height="655" width="655" alt="Recurrent Squeeze-and-Excitation Net for Single Image Deraining">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Recurrent Squeeze-and-Excitation Net for Single Image Deraining</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose a novel deep network architecture based on deep convolutional and recurrent neural networksfor single image deraining.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 19, 2018</p>
    </div>
  </div>
</a>

  

  </div>


  


</div>


    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

  












  
  
  
  
  














  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by text-center">
    © 2024 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by text-center">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>

</footer>

    </div>

    
    











  </body>
</html>
